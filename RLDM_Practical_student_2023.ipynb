{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd5567a",
   "metadata": {},
   "source": [
    "# Bandits and gridworlds\n",
    "\n",
    "Franz Wurm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f321a",
   "metadata": {},
   "source": [
    "This is the code for the practical session in the course \"2223-S2 Reinforcement Learning & Decision-Making: Computational & Neural Mechanisms\" [(link to Brightspace)](https://brightspace.universiteitleiden.nl/d2l/home/215293)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f42bff",
   "metadata": {},
   "source": [
    "**Useful references**\n",
    "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce7d02",
   "metadata": {},
   "source": [
    "## 1. General information\n",
    "\n",
    "### 1.1 Goal of this practical\n",
    "\n",
    "In this practical, you are going to implement a reinforcement learning algorithm for two standard problems.\n",
    "\n",
    "In the [bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit), the agent is faced with a choice between two options. Named after slot machines in a casino, our two-armed version helps to demonstrate the the two core principles that allow the agent to achieve his goal of reward maximzation. \n",
    "\n",
    "Afterwards, we will also investigate reinforcement learning in a more realistic context. In the gridworld problem, the agent is again faced with choices between different options. However, in contrast to the bandit task, choices are sequential, feedback is scarce and the state space big. This poses a bigger problem for learning in the gridworld.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-cleaners",
   "metadata": {},
   "source": [
    "### 1.2 Setting up this notebook \n",
    "\n",
    "This notebook contains all necessary information for the first practical session.\n",
    "\n",
    "I recommend to download the file and save it to a separate folder. Optimally, this folder is easily accessible (e.g. on your desktop) or implemented in a preexisting folder structure (e.g. MyMaster>RLDM>PracticalRL).\n",
    "\n",
    "I also recommend to work with copies. That means, you should not work on the original file, but rather work on copies. This makes sure, that you do not delete important information and always have a basis to go back to. Additionaly, you could implement version control, meaning you save your work to a new file from time to time (e.g., filename_v1, filename_v2). This makes sure you dont lose too much progress in case your computer shuts down or you forgot to press the save button.\n",
    "\n",
    "The exercise is constructed to be completed chronologically, i.e. from top to bottom. There will be blocks with text, such as the text that you are currently reading, as well as blocks of code, where you can execute commands and computations.\n",
    "\n",
    "The whole tutorial is written in R.\n",
    "\n",
    "For working on the excersises of the practical I suggest you use [Google Colab](https://colab.research.google.com/) or [Jupyter-notebook](https://jupyter.org/). While Google Colab is straight foward to use, Jupyter-notebook might require you to install additional software (e.g., [Anaconda](https://anaconda.org/))\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: TODOs </h2><br>\n",
    "\n",
    "In the yellow boxes, I have specified TODOs. Please only continue with the notebook once you have solved each TODO\n",
    "    \n",
    ">HINT: To solve problems, use a <code>?</code> to learn what a command does (e.g., <code>?print</code>), search on the internet or ask the workgroup teacher. If you are really stuck and/or working from home, there are also solutions to this notebook online.\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c5d65",
   "metadata": {},
   "source": [
    "## 2. k-armed bandit task\n",
    "\n",
    "As discussed in the lecture, the [bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit) is a common test bed for both human and AI research. In a bandit task, the agent is faced with a choice between multiple options. Named after slot machines in a casino, the bandit task will help to demonstrate the two core principles that allow the agent to achieve his goal of reward maximzation.\n",
    "\n",
    "The first thing we will do is to create a _function_ that generates samples from playing such a bandit. \n",
    "\n",
    "The function below samples reward drawn from a uniform distribution based on prespecified reward probabilities. As you can see, the function takes input parameters that dictate the number of trials to sample (<code>n_trials</code>) and the reward probablities (<code>probs</code>). \n",
    "\n",
    "The function returns and array with reward samples (<code>reward_samples</code>) and the reward probabilities (<code>reward_probs</code>) that generated it. Please note, that specifying more than one reward probabilities (e.g., <code>[0.1, 0,9]</code>) will sample two independent bandits and return them as a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_binomial_bandit = function(ntrl,probs){\n",
    "    x = sapply(probs,function(p) rbinom(ntrl,1,p))\n",
    "    #p = t(matrix(1,length(probs),ntrl)*probs) #probabilites for samples, might be handy later              \n",
    "    return(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-economy",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: Printing and plotting </h2><br>\n",
    "\n",
    "Put the function to use and check out what it does, using the <code>print</code> and/or <code>plot</code> command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-printer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checkout what the function does\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-emerald",
   "metadata": {},
   "source": [
    "As discussed in the lecture, the two principles of reinforcement learning are\n",
    "- the learning rule: updating of expectations based on observations \n",
    "- the decision rule: taking actions based on expectations\n",
    "\n",
    "\n",
    "## 3. Learning rule \n",
    "\n",
    "The learning rule should help our agent to build up expectations about its environment.\n",
    "\n",
    "In this section, we are going to solve a very simple problem (calculating the mean) using a reinforcement learning algorithms. Afterwards, we will investigate how we can improve this algorithm to be more flexible (e.g., in changing environments)\n",
    "\n",
    "### 3.1 Calculating the average in a stable environment\n",
    "\n",
    "This example is based on Sutton & Barto (2018), Chapter 2.2 Multi-armed bandits (p.27f). For simplicity, I have omitted the iportance of action. Instead of a control problem, we will first consider only the prediction problem.\n",
    "\n",
    "Calculating the average reward $\\hat R _{N}$ is a very straightforward task usually expressed in the simple formula \n",
    "\n",
    "$\\hat R _{N} = \\frac{1}{N} \\sum \\limits_{i=1} ^{N}R _{i} $.\n",
    ">where $\\hat R$ is the average reward,<br>\n",
    ">and $R $ is a vector with $N $ observed rewards.<br>\n",
    "\n",
    "However, this method has a few drawbacks. Importantly, it can not consider new data without recalculating the whole sum. Additionally, with increasing N the working memory load increases as you need to store a record for every number.\n",
    "\n",
    "Fortunately, this is not necessary using an reinforcement learning implementation.\n",
    "\n",
    "We can reformulate our expectation about an upcoming reward\n",
    "\n",
    "$ Q _{N+1} = \\frac{1}{N} \\sum \\limits_{i=1} ^{N}R _{i} = Q _{N} + \\frac{1}{N} [R _{N} - Q _{N}]$.\n",
    ">where $Q$ is the estimated average value,<br>\n",
    ">and $N$ is the number of past observations.<br>\n",
    "\n",
    "You can clearly see the notion of a prediction error: reward minus expected reward in square brackets.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: Complete the learning rule </h2><br>\n",
    "\n",
    "Let's try to implement this algorithm in a stable environement. In the cell below I have already implemented a loop that goes trough each trial. Complete the learning rules, as defined above.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrial = 1000 #number of trials\n",
    "probs = c(0.7) #reward probabilities for independent bandits\n",
    "# TODO: generate a bandit\n",
    "\n",
    "# complete the learning rule\n",
    "Q = 0 #initialize with zero expectation\n",
    "for (iT in 1:nTrial) {\n",
    "    Q = Q\n",
    "}\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-clearing",
   "metadata": {},
   "source": [
    "### 3.2 New bandit and visualization\n",
    "We can also define a new type of bandit where we sample from a normal distribution with mean mu and deviation sigma. In contrast to the previous bandit, the current bandit will draw its reward from a normal distribution. So we will not specify reward probabilities (<code>probs</code>) but rather reward means (<code>mu</code>) and respective standard deviations (<code>sigma</code>) as the second and third argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_normal_bandit = function(ntrl,mu,sigma){\n",
    "    x = t(matrix(rnorm(ntrl*length(mu),mu,sigma),length(mu),ntrl))\n",
    "    return(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkout what the function does\n",
    "nTrial = 1000 #number of trials\n",
    "mu = c(1,1.5,2) #reward probabilities for independent bandits\n",
    "sigma = c(0.1, 0.5, 0.2)\n",
    "r = generate_normal_bandit(nTrial,mu,sigma)\n",
    "#print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-intake",
   "metadata": {},
   "source": [
    "For this example, visualization is a bit more tricky. Therefore, I have already written the code. Please try to understand what has been done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = floor(min(r))\n",
    "c = ceiling(max(r))\n",
    "#print(b)\n",
    "#print(c)\n",
    "\n",
    "ax = pretty(b:c,100)\n",
    "#print(ax)\n",
    "\n",
    "c1 <- rgb(173,216,230,max = 255, alpha = 80, names = \"lt.blue\")\n",
    "c2 <- rgb(255,192,203, max = 255, alpha = 80, names = \"lt.pink\")\n",
    "c3 <- rgb(100,255,100, max = 255, alpha = 80, names = \"lt.green\")\n",
    "\n",
    "\n",
    "hgA = hist(r[,1],breaks = ax, plot = FALSE)\n",
    "hgB = hist(r[,2],breaks = ax, plot = FALSE)\n",
    "hgC = hist(r[,3],breaks = ax, plot = FALSE)\n",
    "\n",
    "plot(hgA, col = c1)\n",
    "plot(hgB, col = c2, add = TRUE)\n",
    "plot(hgC, col = c3, add = TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-charge",
   "metadata": {},
   "source": [
    "Let's try the same for this bandit. \n",
    "\n",
    "Using the same averaging method from aboce, we should find again, that the estimated mean <code>Q</code> approaches the true mean <code>mu</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrial = 1000 #number of trials\n",
    "mu = c(1) #reward probabilities for independent bandits\n",
    "sd = c(0.1)\n",
    "r = generate_normal_bandit(nTrial,mu,sd)\n",
    "\n",
    "Q = 0 #initialize with zero expectation\n",
    "for (iT in 1:nTrial) {\n",
    "    Q = Q + (1/iT)*(r[iT]-Q)\n",
    "}\n",
    "cat('True mean: ')\n",
    "print(mu)\n",
    "cat('Estimated mean: ')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-badge",
   "metadata": {},
   "source": [
    "Now let's try to visualize how the estimated value approaches the true value.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: Temporal progression </h2><br>\n",
    "\n",
    "The estimate for expected reward is slowly progressing towards the true mean. Visualize this progression.\n",
    "    \n",
    ">HINT: Use the <code>matrix</code> command to preallocate an (empty) matrix and then fill it step by step.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: temporal progression + plotting\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-manner",
   "metadata": {},
   "source": [
    "### 3.3 Calculating the average in a changing environment\n",
    "\n",
    "Now that we have seen that the average method of incremental learning does a good job in estimating the average on a step-by-step basis, we are going to extend this example to cover changing environments. \n",
    "\n",
    "In our new bandit task, the environment undergoes sudden changes in reward probabilities. For simplicity, lets get back to our bandit with a binomial reward distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrial = 100 #number of trials\n",
    "probs = c(1,0,1,0,1,0) #reward probabilities for independent bandits\n",
    "r = generate_binomial_bandit(nTrial,probs)\n",
    "#print(r)\n",
    "\n",
    "# this is a workaround! \n",
    "# instead of making a bandit function with changing rewards, \n",
    "# we simply simulate multiple independent bandits\n",
    "# and then flatten the matrix\n",
    "nT = (nTrial*length(probs))\n",
    "Q = matrix(,nT+1,1)\n",
    "rflat = as.vector(r)\n",
    "ptrue = t(matrix(1,length(probs),nTrial)*probs) #probabilites for samples in matrix form\n",
    "pflat = as.vector(ptrue)\n",
    "\n",
    "\n",
    "Q[1,1]=0.5\n",
    "for (iT in 1:nT) {\n",
    "    Q[iT+1,1] = Q[iT,1] + (1/iT)*(r[iT]-Q[iT,1])\n",
    "}\n",
    "\n",
    "\n",
    "#print(Q)\n",
    "plot(NA, ylim = c(0, 1), xlim = c(1, nT+1), main = \"Estimated mean\",\n",
    "     xlab = \"Trial\",\n",
    "     ylab = \"p(reward)\") #better view over the lines\n",
    "\n",
    "lines(1:(nT+1),Q, type = \"l\", col = 'red',lwd=2) #thickness set to be higher\n",
    "lines(1:(nT),pflat, type = \"l\", col = 'black',lwd=2) #thickness set to be higher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-armor",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: Question </h2><br>\n",
    "\n",
    "What is going on here? Instead of following the true mean, the estimated mean is slowly approaching 0.5. Why is that the case? Is it an error? How could we solve the problem?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explain and find a solution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-handle",
   "metadata": {},
   "source": [
    "**Answer:** While this seems to be odd, it is exactly what we told our algorithms to do: calculate the average over all oberservations. Alternating between 0 and 1, finally leads to an average of 0.5 But how do we fix this issue?\n",
    "\n",
    "The solution lies in the scaling parameter, with which scale our update using the prediction error -  the so called learning rate $ \\alpha $. So far, we have implemented a learning rate that scales with the number of trials, so that with more and more trials, new data gets less and and less impact on the overall average. However, in a changing environment we want exactly that new information actually does have an impact on our estimation.\n",
    "\n",
    "Introducing a fixed learning rate solves this problem. Mathematically, a fixed learning rate balances the importance of old and new information, effectively introducing a memory. Old information is forgotten (i.e., down-weighted) while new information is incorporated. The respective formula is \n",
    "\n",
    "$ Q _{N+1} = Q _{N+1} + \\alpha [R _{N} - Q _{N}]$,\n",
    "\n",
    "with learning rate $ \\alpha $ between 0 and 1. Stressing the memory analogy, it is clear what the extreme values do. A learning rate of 0 means that the algorithm solely focussed on old information, i.e., no incorporating any new information. A learning rate of 1 means that the algorithm solely focussed on new information, i.e., discarding all previous information.\n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "Q[1,1]=0.5\n",
    "for (iT in 1:nT) {\n",
    "    Q[iT+1,1] = Q[iT,1] + alpha*(r[iT]-Q[iT,1])\n",
    "}\n",
    "\n",
    "\n",
    "#print(Q)\n",
    "plot(NA, ylim = c(0, 1), xlim = c(1, nT+1), main = \"Estimated mean\",\n",
    "     xlab = \"Trial\",\n",
    "     ylab = \"p(reward)\") #better view over the lines\n",
    "\n",
    "lines(1:(nT+1),Q, type = \"l\", col = 'red',lwd=2) #thickness set to be higher\n",
    "lines(1:(nT),pflat, type = \"l\", col = 'black',lwd=2) #thickness set to be higher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-costs",
   "metadata": {},
   "source": [
    "## 4. Decision rule\n",
    "\n",
    "So far, we have only dealt with prediction problems. However, the most powerful application of reinforcement learning lies in solving control problems. \n",
    "\n",
    "The decision rule (or policy) defines the agent's way of behaving at a given time or state. This is and extension of the previous prediction problem, as we now also consider actions given each state. Put differently, the policy is a mapping between states and actions and it corresponds to what psychologists sometimes call a stimulus-response association.\n",
    "\n",
    "### 4.1 Action selection methods for decision-making\n",
    "\n",
    "In order to make decisions, we need to have options. Our bandit function are already perfectly suited for this type of problem. All we need now is a function that takes actions based on estimated outcomes.\n",
    "\n",
    "During the lecture, we introduced a few different decision rules and of course the list in not exhaustive. Below you find the most commonly used methods:\n",
    "- random method\n",
    "- greedy method\n",
    "- e-greedy method\n",
    "- softmax method\n",
    "\n",
    "Let's first implement the different methods for action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_random = function(values) {\n",
    "    nOptions = length(values)\n",
    "    action = sample(nOptions,1)\n",
    "    return(action)\n",
    "}\n",
    "\n",
    "action_greedy = function(values) {\n",
    "    actions = 1:length(values)\n",
    "    p = matrix(1,1,length(values))\n",
    "    maxactions = which(values == max(values))\n",
    "    if (length(maxactions)==1) {\n",
    "        action = maxactions\n",
    "    } else { #special case if there are identical values\n",
    "        action = sample(actions[maxactions],1)\n",
    "    }\n",
    "    return(action)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-transport",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: $\\epsilon$-greedy </h2><br>\n",
    "    \n",
    "Implement another action selection method: Epsilon greedy\n",
    "    \n",
    ">HINT: Use the functions specified above to make your task easier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_egreedy = function(values,epsilon) {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-hungary",
   "metadata": {},
   "source": [
    "!!! challenge: implement epsilon greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-gossip",
   "metadata": {},
   "source": [
    "### 4.2 The importance of exploration\n",
    "\n",
    "Now that we have a decision-making algorithm, we can combine both learning and decision making and then simulate our first reinforcement learning algorithm.\n",
    "\n",
    "But let's make a few assumptions for our simulation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "nAgents = 100\n",
    "nTrials = 1000\n",
    "nOptions = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-trinity",
   "metadata": {},
   "source": [
    "#### 4.2.1 Greedy agents\n",
    "\n",
    "In the following code block I have set up a number of greedy agents.\n",
    "\n",
    "We will save both the average for the rewards (<code>rewards_greedy</code>) and the percentage of optimal decisions (<code>optimum_greedy</code>) will be saved for plotting.\n",
    "\n",
    "Please note, that we implement the _average methode_ as the learning rule. As this is a stable environment, the properties of the average method guarantee that our estimates converge to the optimum/true value (at least with an appropriate action selection method...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-wednesday",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rewards_greedy = matrix(,nTrials,nAgents)\n",
    "optimum_greedy = matrix(,nTrials,nAgents)\n",
    "\n",
    "for (iA in 1:nAgents) {\n",
    "    reward_means = rnorm(nOptions,0,1)\n",
    "    optimal_action = which.max(reward_means)    \n",
    "    #print(reward_means)\n",
    "    #print(optimal_action)\n",
    "    \n",
    "    r = generate_normal_bandit(nTrials,reward_means,1)    \n",
    "    #print(r)\n",
    "    \n",
    "    #initialize the estimated values for each bandit\n",
    "    Q = matrix(0,nTrials+1,nOptions)\n",
    "    actioncounter = matrix(0,1,nOptions)\n",
    "\n",
    "     for (iT in 1:nTrials) {\n",
    "        action = action_greedy(Q[iT,])\n",
    "        actioncounter[1,action] = actioncounter[1,action]+1\n",
    "        Q[iT+1,] =  Q[iT,] #important to forward past expectations\n",
    "        Q[iT+1,action] = Q[iT,action] + (1/actioncounter[1,action])*(r[iT,action]-Q[iT,action])\n",
    "        \n",
    "        rewards_greedy[iT,iA] = r[iT,action]\n",
    "        if (action == optimal_action) {\n",
    "            optimum_greedy[iT,iA] = 1\n",
    "        } else {\n",
    "            optimum_greedy[iT,iA] = 0\n",
    "        }            \n",
    "    }\n",
    "    #print(optimal_action)\n",
    "    #print(actioncounter)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-moldova",
   "metadata": {},
   "source": [
    "#### 4.2.2 $\\epsilon$-greedy action selection\n",
    "\n",
    "Basically, the $\\epsilon$-greedy agent is identical to the greedy agent with the the minor detail that on 100*$\\epsilon$% of the trials a random action will be chosen.\n",
    "\n",
    "You already worked ouy the greedy method so let's put it to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "rewards_egreedy = matrix(,nTrials,nAgents)\n",
    "optimum_egreedy = matrix(,nTrials,nAgents)\n",
    "\n",
    "for (iA in 1:nAgents) {\n",
    "    reward_means = rnorm(nOptions,0,1)\n",
    "    optimal_action = which.max(reward_means)    \n",
    "    #print(reward_means)\n",
    "    #print(optimal_action)\n",
    "    \n",
    "    r = generate_normal_bandit(nTrials,reward_means,1)    \n",
    "    #print(r)\n",
    "    \n",
    "    #initialize the estimated values for each bandit\n",
    "    Q = matrix(0,nTrials+1,nOptions)\n",
    "    actioncounter = matrix(0,1,nOptions)\n",
    "\n",
    "     for (iT in 1:nTrials) {\n",
    "        action = action_egreedy(Q[iT,],epsilon)\n",
    "        actioncounter[1,action] = actioncounter[1,action]+1\n",
    "        Q[iT+1,] =  Q[iT,] \n",
    "        Q[iT+1,action] = Q[iT,action] + (1/actioncounter[1,action])*(r[iT,action]-Q[iT,action])\n",
    "        \n",
    "        rewards_egreedy[iT,iA] = r[iT,action]\n",
    "        if (action == optimal_action) {\n",
    "            optimum_egreedy[iT,iA] = 1\n",
    "        } else {\n",
    "            optimum_egreedy[iT,iA] = 0\n",
    "        }            \n",
    "    }   \n",
    "    #print(optimal_action)\n",
    "    #print(actioncounter)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-anthropology",
   "metadata": {},
   "source": [
    "Now we can plot the performance for each action selection method.\n",
    "\n",
    "### 4.2.3 Comparison (via plotting)\n",
    "\n",
    "First, we plot the collected reward, based on the average over all agents.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: Question </h2><br>\n",
    "Before we continue, think about the performance of our agents. Do you have any hypothesis about the difference between epsilon greedy and greedy action selection? What would you expect from the RL agents in general? How should their behavior change?\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answering the question\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-stations",
   "metadata": {},
   "source": [
    "Execute the code, once you have answered the question, and observe what happend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-alfred",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(NA, ylim = c(-2, 2), xlim = c(1, nTrials), main = \"Estimated mean\",\n",
    "     xlab = \"Trial\",\n",
    "     ylab = \"average reward\") #better view over the lines\n",
    "\n",
    "lines(1:nTrials,rowMeans(rewards_greedy), type = \"l\", col = 'red',lwd=2) #thickness set to be higher\n",
    "lines(1:nTrials,rowMeans(rewards_egreedy), type = \"l\", col = 'black',lwd=2) #thickness set to be higher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-insulin",
   "metadata": {},
   "source": [
    "Next we can also plot the perentage of optimal choice, based on the average over all agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(NA, ylim = c(0, 1), xlim = c(1, nTrials), main = \"Estimated mean\",\n",
    "     xlab = \"Trial\",\n",
    "     ylab = \"average reward\") #better view over the lines\n",
    "\n",
    "lines(1:nTrials,rowMeans(optimum_greedy), type = \"l\", col = 'red',lwd=2) #thickness set to be higher\n",
    "lines(1:nTrials,rowMeans(optimum_egreedy), type = \"l\", col = 'black',lwd=2) #thickness set to be higher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-mistress",
   "metadata": {},
   "source": [
    "### Softmax action selection\n",
    "\n",
    "Of course, those 3 methods are not the only action selectino methods. Let's have a closer look the softmax rule, as this is maybe the most widely used decision rule in the neuroscientific literature.\n",
    "\n",
    "$\\LARGE p(a)= \\frac{e ^{(\\beta * Q(a))}} {\\sum \\limits _{a'} e ^{(\\beta * Q(a'))}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-bench",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: Softmax function </h2><br>\n",
    "\n",
    "The central part of the softmax algorithm is missing.\n",
    "    \n",
    ">HINT: Use prespecified methods like <code>sum</code> or <code>exp</code>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = function(Q,beta) {\n",
    "  p = FALSE\n",
    "  return(p)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-stanford",
   "metadata": {},
   "source": [
    "Compared to the previous action selection method, the softmax method works a bit different. We do not immediately make an action, but rather transfer action values into action probabilities first.\n",
    "\n",
    "Putting this softmax function to use, we need to make a few hypothetical assumptions for our bandit task.\n",
    "\n",
    "For example, let's assume our agent has played the slot machine twice, each arm one time. The left arm lead to a reward (1), whereas the right arm did not result in a reward (0). We can translate this experience into simplified expectations for the next game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qdummy = c(1,0)\n",
    "beta = 1\n",
    "\n",
    "p = softmax(Qdummy,beta)\n",
    "\n",
    "cat('beta: ')\n",
    "print(beta)\n",
    "cat('Expectations: ')\n",
    "print(Qdummy)\n",
    "cat('Probabilities: ')\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-lithuania",
   "metadata": {},
   "source": [
    "We can see, that the dummy expectations translate into action probabilites in a straightforward way. The highest value has the highest action probability.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> TODO: Question </h2><br>\n",
    "\n",
    "What is the role of beta in the softmax function?\n",
    "    \n",
    ">HINT: Play around with the beta value and observe the changes.\n",
    "\n",
    "</div>\n",
    "\n",
    "You can write your answers below and discuss it with your colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the role of beta in the softmax function?\n",
    "# (you can change this code block to markdown)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-stability",
   "metadata": {},
   "source": [
    "Once you have answered the above question, you can play around with the beta value and observe the changes!\n",
    "\n",
    "You will see, that beta (or 'inverse temperature') affects the so-called [gain](https://en.wikipedia.org/wiki/Gain_(electronics)). The higher the gain, the more pronounced the differences in action values get translated into action probabilities.\n",
    "\n",
    "We can plot this in a systematic way.\n",
    "Try to understand what is plotted on the x and y axis, and how the different betas relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(NA, ylim = c(0, 1), xlim = c(-2, 2), main = \"Softmax for 2 options\",\n",
    "     xlab = \"Q(A)-Q(B)\",\n",
    "     ylab = \"p(A)\") #better view over the lines\n",
    "x = seq(-2,2,0.05)\n",
    "betas = c(0,1,2,5,10)\n",
    "mypal <- colorRampPalette( c( \"red\", \"green\", \"blue\") )( length(betas) )\n",
    "for (i in 1:length(betas)) {\n",
    "  y = integer(length(x))\n",
    "  for (j in 1:length(x)){\n",
    "    p = softmax(Qdummy*x[j],betas[i])\n",
    "    y[j] = p[1]\n",
    "  }\n",
    "  lines(x,y, type = \"l\", col = mypal[i], , lwd=1.5) #thickness set to be higher\n",
    "}\n",
    "legend(\"topleft\", legend=as.character(betas), lwd = 1,col=mypal,title='betas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-speed",
   "metadata": {},
   "source": [
    "In the next step, we can finally put the softmax into action.\n",
    "\n",
    "Before we can make a decision, we need to define the action function for the softmax method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_softmax = function(values,beta) {\n",
    "    actions = length(values)\n",
    "    p = softmax(values,beta)\n",
    "    action = sample(actions,size = 1, prob = p)\n",
    "    return(action)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-biology",
   "metadata": {},
   "source": [
    "Afterwards, we can use the method and compare it to greedy action selection in our stable environment with normally distributed rewards.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Softmax and greedy action selection </h2><br>\n",
    "\n",
    "Based on the schematic figure for different betas and the simulation above, how does greedy, random and softmax action selection relate?\n",
    "    \n",
    ">Task: Can you get the softmax method to show the same results as greedy or random action selection?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.8\n",
    "\n",
    "rewards_softm = matrix(,nTrials,nAgents)\n",
    "optimum_softm = matrix(,nTrials,nAgents)\n",
    "\n",
    "for (iA in 1:nAgents) {\n",
    "    reward_means = rnorm(nOptions,0,1)\n",
    "    optimal_action = which.max(reward_means)    \n",
    "    #print(reward_means)\n",
    "    #print(optimal_action)\n",
    "    \n",
    "    r = generate_normal_bandit(nTrials,reward_means,1)    \n",
    "    #print(r)\n",
    "    \n",
    "    #initialize the estimated values for each bandit\n",
    "    Q = matrix(0,nTrials+1,nOptions)\n",
    "    actioncounter = matrix(0,1,nOptions)\n",
    "\n",
    "     for (iT in 1:nTrials) {\n",
    "        action = action_softmax(Q[iT,],beta)\n",
    "        actioncounter[1,action] = actioncounter[1,action]+1\n",
    "        Q[iT+1,] =  Q[iT,] \n",
    "        Q[iT+1,action] = Q[iT,action] + (1/actioncounter[1,action])*(r[iT,action]-Q[iT,action])\n",
    "        \n",
    "        rewards_softm[iT,iA] = r[iT,action]\n",
    "        if (action == optimal_action) {\n",
    "            optimum_softm[iT,iA] = 1\n",
    "        } else {\n",
    "            optimum_softm[iT,iA] = 0\n",
    "        }            \n",
    "    }   \n",
    "    #print(optimal_action)\n",
    "    #print(actioncounter)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(NA, ylim = c(-2, 2), xlim = c(1, nTrials), main = \"Estimated mean\",\n",
    "     xlab = \"Trial\",\n",
    "     ylab = \"average reward\") #better view over the lines\n",
    "\n",
    "lines(1:nTrials,rowMeans(rewards_greedy), type = \"l\", col = 'red',lwd=2) #thickness set to be higher\n",
    "lines(1:nTrials,rowMeans(rewards_softm), type = \"l\", col = 'green',lwd=2) #thickness set to be higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list = ls()) #clear the workspace before we continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52476bc",
   "metadata": {},
   "source": [
    "### The grid world problem\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Additional material </h2><br>\n",
    "\n",
    "This is additional and optional material.\n",
    "    \n",
    "</div>\n",
    "\n",
    "*Grid worlds* are simplified respresentations of the environment, that are often used for navigation simulation. As is implied by the name, grid worlds break down the environment into a grid, similar to a chess board. For the scope of this course we are going to stick with a very basic 4x4 gridworld.\n",
    "\n",
    "Before we dive further into the code, a bit more background information on gridworlds. \n",
    "\n",
    "Gridworlds are so-called [Markov Decision Processes](https://en.wikipedia.org/wiki/Markov_decision_process). In contrast to the previous bandit task, gridworlds usually are multi-step problems, meaning that actions executed in one state cannot only result in reward, but also affect the upcoming state. This necessitates the agent to not only consider the immediate reward but also the expected cumulative reward. \n",
    "\n",
    "Our agent always starts in the same start state (s_0, top left of board). From there, it will take *steps*, that gradually move him across the board. Movement is restricted to the cardinal directions (up, down, right, left). Reward is located in the terminal state (s_terminal, bottom right of board). Upon arrival at the site of reward, the agent receives the reward (associated with a positive value) and will be returned to the initial state, so the whole procedure can start again. The (time)steps between start and terminal state are regarded as a *run* (or *episode*). Start state, reward and terminal state do NOT change between runs. Thus, our gridworld environment is stable.\n",
    "\n",
    "For this grid world example, we will implement the Q learning rule, which is defined as\n",
    "\n",
    "$\\LARGE Q_{new}(s,a) = Q_{old}(s,a) + \\alpha * (R + \\gamma max_{a} Q_{old}(s',a) - Q_{old}(s,a))$,\n",
    "\n",
    "    where $\\alpha$ is the learning rate,\n",
    "    $\\gamma$ is the discounting factor,\n",
    "    and s' is the next state\n",
    "\n",
    "Below, we define the most basic details for our gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied function from bandit\n",
    "takeAction = function(values,options) {\n",
    "    if (length(values) == length(options)) {\n",
    "        p = softmax(values,beta)\n",
    "        action = sample(options, size = 1, prob = p)\n",
    "    } else {### The grid world problem\n",
    "\n",
    "# copied function from bandit\n",
    "softmax = function(Q,beta) {\n",
    "  p = exp(beta * Q) / sum(exp(beta * Q))\n",
    "  return(p)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 4\n",
    "\n",
    "s_0 <- 0 # start at initial state\n",
    "s_terminal <- (grid_size^2)-1 # goal state\n",
    "actions <- c(\"left\", \"up\", \"right\", \"down\")\n",
    "\n",
    "# assign numbers to each state in the gridworld\n",
    "states = matrix(, nrow = grid_size, ncol = grid_size)\n",
    "iZ = 0\n",
    "for (iX in 1:grid_size) {\n",
    "  for (iY in 1:grid_size) {\n",
    "    states[iY, iX] = iZ\n",
    "    iZ = iZ + 1\n",
    "  }\n",
    "}\n",
    "\n",
    "print(paste(\"state state: \",s_0))\n",
    "print(paste(\"final state: \",s_terminal))\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "getReward_grid <- function(state) {\n",
    "  if (state == s_terminal) {\n",
    "    reward <- 100\n",
    "  } else {\n",
    "    reward <- -1\n",
    "  }\n",
    "  return(reward)\n",
    "}\n",
    "\n",
    "makeMove <- function(sin, ain) {\n",
    "  sout = sin \n",
    "  coords = which(states == sin, arr.ind = TRUE)  \n",
    "  if (ain == \"down\")\n",
    "    coords[1] <- coords[1] + 1\n",
    "  if (ain == \"up\")\n",
    "    coords[1] <- coords[1] - 1\n",
    "  if (ain == \"right\")\n",
    "    coords[2] <- coords[2] + 1\n",
    "  if (ain == \"left\")\n",
    "    coords[2] <- coords[2] - 1\n",
    "  \n",
    "  if (coords[1] < 1)\n",
    "    coords[1] = 1\n",
    "  if (coords[1] > length(states[, 1]))\n",
    "    coords[1] = length(states[, 1])\n",
    "  if (coords[2] < 1)\n",
    "    coords[2] = 1\n",
    "  if (coords[2] > length(states[1, ]))\n",
    "    coords[2] = length(states[1, ])\n",
    "  \n",
    "  sout = states[coords]  \n",
    "  return(sout)\n",
    "}\n",
    "\n",
    "TDlearn_grid = function(values,reward,action,state,nextstate) {\n",
    "    values[state,action] = values[state,action] + alpha * (reward + max(values[nextstate,]) - values[state,action])\n",
    "    return(values)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c9657",
   "metadata": {},
   "source": [
    "Now that we have defined the most crucial parts for our grid world, we can put everything together.\n",
    "\n",
    "For convenience, I have already added multiple agents, as this will allow us to draw more precise conclusions from the simulation data.\n",
    "\n",
    "For the agent's parameters I have implemented very basic values. You can have a look how changing those parameters affects the performance in the grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082ec97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nAgent = 20\n",
    "nRun = 100\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 1\n",
    "\n",
    "movecounter = matrix(0, nrow = nRun, ncol = nAgent)\n",
    "Qall = array(0,c(grid_size^2, length(actions), nAgent))\n",
    "\n",
    "for (iAgent in 1:nAgent) {\n",
    "    \n",
    "#     print(paste(\"agent \",iAgent))\n",
    "    Q <- array(0, c(grid_size^2, length(actions)))\n",
    "\n",
    "    for (iRun in 1:nRun) {\n",
    "    \n",
    "#         if (iRun %% 20 == 0) {\n",
    "#           print(paste(\"   trial \",iRun))\n",
    "#         }\n",
    "    \n",
    "        state <- s_0 # set cursor to initial state\n",
    "        sidx = which(states == state)\n",
    "    \n",
    "        moves = 0   \n",
    "        while (state != s_terminal) {\n",
    "        \n",
    "            action = takeAction(Q[sidx,],actions)\n",
    "            next_state <- makeMove(state, action)\n",
    "            reward <- getReward_grid(next_state)\n",
    "        \n",
    "            aidx = which(actions == action)\n",
    "            s2idx = which(states == next_state)\n",
    "            Q = TDlearn_grid(Q,reward,aidx,sidx,s2idx)\n",
    "        \n",
    "            response <- (list(state = next_state, reward = reward))\n",
    "\n",
    "      \n",
    "#             if ((iRun == 1) & (next_state == s_terminal))\t{\n",
    "#                print(paste(\"trial \",iRun))\n",
    "#                print(Q) # what does q-look like after first successful iteration (i==1)\n",
    "#             }\n",
    "#             if ((iRun == nRun) & (next_state == s_terminal))\t{\n",
    "#                print(paste(\"trial \",iRun))\n",
    "#                print(Q) # what does q-look like after first successful iteration (i==1)\n",
    "#             }\n",
    "          \n",
    "            state <- response$state # move to next state\n",
    "            sidx = which(states == state)\n",
    "      \n",
    "            moves = moves + 1\n",
    "        }\n",
    "        Qall[,,iAgent] = Q\n",
    "        movecounter[iRun, iAgent] = moves\n",
    "    }\n",
    "}\n",
    "#print(movecounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7b4ed",
   "metadata": {},
   "source": [
    "In order to better understand the agents' behavior in the gridworld we can again make use of the `print()` and `plot()` function.\n",
    "\n",
    "**Question**: As a first step, lets have a look at the performance of our agent. Do you have any hypothesis about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e02862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is your hypothesis?\n",
    "# (you can change this code block to markdown)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be250d",
   "metadata": {},
   "source": [
    "**Answer**: Performance for learning agents should improve over time. Therefore we should expect to find improvements in performance for our grid world agent. Performance can be measured in multiple different ways. For our grid world example, we could define 2 measures of preformance. The first measure is already plotted below: the number of steps it takes the agents to get from start state to terminal state. While the agent is aimlessly roaming the gridworld in the beginning, it quickly picks up the optimal path to the goal.\n",
    "\n",
    "**Excercise 2** Define a second measure of performance, implement it in the code and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d62ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting performance\n",
    "plot(1:nRun, apply(movecounter, c(1), mean), xlab = \"Runs\", ylab = \"Steps\", main=\"Performance\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d87b67",
   "metadata": {},
   "source": [
    "Finally, we can also have a look at the policy of our agents.\n",
    "\n",
    "Plotting the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions2plot <- c(\"left \", \" up  \", \"right\", \"down \")\n",
    "\n",
    "\n",
    "a = apply(Qall, c(1,2), mean)\n",
    "b = apply(a, c(1), which.max)\n",
    "dim(b) = c(grid_size,grid_size)\n",
    "\n",
    "#print(a)\n",
    "\n",
    "print(\"policy for recent agent\")\n",
    "for (i in 1:grid_size) {\n",
    "  cat('|')\n",
    "  for (j in 1:grid_size) {\n",
    "    if (states[i,j]==s_0) {\n",
    "      cat(paste('*',actions2plot[b[i,j]],'*|'))\n",
    "    } else if (states[i,j]==s_terminal) {\n",
    "      cat(paste('+',actions2plot[b[i,j]],'+|'))\n",
    "    } else {\n",
    "      cat(paste(' ',actions2plot[b[i,j]],' |'))\n",
    "    }\n",
    "  }\n",
    "  print('')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51ec39",
   "metadata": {},
   "source": [
    "**Excercise 3.** So far, we have only investigated the softmax and random method for our gridworld, with random action selection being a special case of softmax action selection. Try to implement alternative methods, such as greedy or e-greedy and have a look how performance changes. \n",
    "\n",
    "**Excercise 4.** The current grid world is perfectly deterministic. Actions reliably lead to the same outcome. Reward is always hidden in the same location. Recall that for our bandit example reward delivery was probabilistic. Can you implement some randomness in the grid world as well? Think about a few possible options. How could you implement those and what would be the consequences for agents' learning and decision-making?\n",
    "\n",
    "**Excercise 5.** (optional) Although we have only discussed different decision rules at this point, we could also think about different learning rules. A prominent example is already described in Sutton & Barto (2018, p132). In their variant of the grid world, learning for the different learning rules translates into very distinct behavioral patterns. It nicely illustrates how small changes in the algorithm can have quite strong effects on decision making. Have a look at the example. Can you construct a cliff world and replicate the findings reported by Sutton & Barto?      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-catalog",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
